# DONNA

Know what happened. Avoid what failed.


**Distributed Organizational Neural Network Assistant**

<div align="center">

_Transforming scattered organizational history into actionable institutional knowledge_

[![Next.js](https://img.shields.io/badge/Next.js-15-black?style=flat-square&logo=next.js)](https://nextjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat-square&logo=fastapi)](https://fastapi.tiangolo.com/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=flat-square&logo=typescript)](https://www.typescriptlang.org/)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat-square&logo=python)](https://www.python.org/)

</div>

---

## The Problem

Organizations suffer from **institutional amnesia**:

- **Repeated Mistakes**: Teams lack visibility into past failures and repeat them
- **Knowledge Loss**: When employees leave, critical context and reasoning disappears
- **Slow Onboarding**: New hires can't understand why systems exist or how they evolved
- **Context Fragmentation**: Critical knowledge is scattered across Slack, GitHub, Jira, Confluence, and Docs

The question "Has anyone tried this before?" echoes through Slack channels, met with silence or vague memories.

---

## The Solution

**DONNA** is an AI-powered institutional memory assistant that captures, contextualizes, and surfaces historical organizational knowledge through conversational exploration of real case studies.

### How DONNA Helps

- **Learns from History**: Analyzes codebases, docs, tickets, and communication tools to understand what worked, what failed, and why
- **Understands Previous Work**: Builds a contextual map of past projects, showing how systems evolved and why certain choices were made
- **Warns Before Mistakes**: Proactively flags decisions similar to past failures with context, impact, and proven solutions
- **Preserves Organizational Wisdom**: Knowledge persists beyond employee tenure, new hires learn from years of experience from day one

---

## Features

### Case-Based Learning

Explore four interactive case workspaces, each representing a real company's historical data including Slack discussions, GitHub commits, Jira tickets, Confluence documentation, and Google Docs notes.

### Conversational AI

Ask DONNA anything about organizational history and receive contextual, AI-generated answers using the company's data as reference.

### Polished Interface

- Elegant black-and-white aesthetic
- Smooth animations via Framer Motion
- Mobile-responsive design
- Collapsible context panels
- Smart query suggestions

### Intelligent Context

- **Vector Search**: Semantic understanding via Qdrant vector database
- **Separate Chat Threads**: Each case maintains its own workspace memory
- **Streamed Responses**: Real-time AI generation for fluid interaction

---

## Architecture

[![Architecture Diagram](./assets/arch.png)](./assets/arch.png)

### Tech Stack

**Frontend**

- Next.js 15 (App Router)
- React 18 + TypeScript
- Tailwind CSS
- Framer Motion
- AI SDK (Vercel)

**Backend**

- FastAPI
- Python 3.12
- MongoDB (company data, metadata)
- Qdrant (vector database)
- Ollama (nomic-embed-text embeddings)
- Google Gemini AI (chat generation)

---

## Getting Started

### Prerequisites

Ensure you have the following installed and running:

- Node.js 18+
- Python 3.12+
- MongoDB instance
- Qdrant vector database (localhost:6333)
- Ollama (with nomic-embed-text model)

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/Manas-Nanivadekar/DONNA.git
   cd DONNA
   git checkout dev
   ```

2. **Set up environment variables**

   Create a `.env.local` file in the root directory:

   ```env
   # Google Gemini AI
   GOOGLE_API_KEY=your_google_api_key_here

   # MongoDB
   MONGODB_URI=mongodb://localhost:27017
   MONGODB_DB_NAME=donna

   # Qdrant
   QDRANT_HOST=localhost
   QDRANT_PORT=6333

   # Ollama
   OLLAMA_BASE_URL=http://localhost:11434
   ```

3. **Install frontend dependencies**

   ```bash
   pnpm install
   # or
   npm install
   # or
   yarn install
   ```

4. **Install backend dependencies**

   ```bash
   # Create and activate virtual environment
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate

   # Install Python packages
   pip install -r requirements.txt
   ```

5. **Start required services**

   ```bash
   # Start MongoDB (if not already running)
   mongod

   # Start Qdrant
   docker run -p 6333:6333 qdrant/qdrant

   # Start Ollama
   ollama serve

   # Pull required embedding model
   ollama pull nomic-embed-text
   ```

6. **Run the application**
   ```bash
   pnpm dev
   ```

The application will be available at:

- Frontend: `http://localhost:3000`
- Backend API: `http://localhost:3000/api/*`

---

## Project Structure

```
DONNA/
├── api/                          # FastAPI Backend
│   ├── index.py                  # Main API endpoint
│   ├── services/
│   │   ├── vector_store.py       # Qdrant vector operations
│   │   ├── embedding.py          # Ollama embeddings
│   │   └── llm.py                # Gemini AI integration
│   └── utils/
│       ├── stream.py             # Response streaming
│       ├── prompt.py             # Prompt engineering
│       └── attachment.py         # File handling
│
├── app/                          # Next.js Frontend
│   ├── (chat)/
│   │   ├── page.tsx              # Main chat page
│   │   └── [caseId]/             # Dynamic case routes
│   ├── cases/
│   │   └── page.tsx              # Case selection landing
│   ├── layout.tsx                # Root layout
│   └── api/                      # API route handlers
│
├── components/                   # React Components
│   ├── chat.tsx                  # Chat interface
│   ├── case-card.tsx             # Case preview cards
│   ├── context-panel.tsx         # Collapsible summary
│   ├── suggestions-panel.tsx     # Query suggestions
│   ├── message.tsx               # Chat message display
│   ├── multimodal-input.tsx      # Chat input field
│   └── ui/                       # Reusable UI components
│
├── lib/
│   ├── db/
│   │   ├── mongodb.ts            # MongoDB client
│   │   └── models.ts             # Data models
│   └── utils.ts                  # Utility functions
│
├── hooks/
│   ├── use-chat-history.tsx      # Per-case chat state
│   └── use-scroll-to-bottom.tsx  # Auto-scroll
│
├── public/
│   └── cases/                    # Case metadata & assets
│
├── requirements.txt              # Python dependencies
├── package.json                  # Node.js dependencies
└── vercel.json                   # Deployment config
```

---

## User Experience Flow

1. **Landing Page**: User sees three case study cards
2. **One-Time Registration**: User provides name and email
3. **Case Selection**: Click a case to enter its workspace
4. **Workspace Interface**:
   - **Left Panel**: Collapsible case summary with full historical context
   - **Center**: Chat interface for conversational exploration
   - **Right Panel**: Suggested queries that prefill the input
5. **Exploration**: Ask questions, receive AI-streamed answers based on company data
6. **Case Switching**: Each case maintains its own separate chat thread

---

## API Endpoints

### `POST /api/chat`

Main chat endpoint for conversational AI

**Request Body:**

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Why did Project Phoenix fail?"
    }
  ],
  "caseId": "company-a",
  "userId": "user-123"
}
```

**Response:**
Streamed text using Data Stream Protocol

### `GET /api/cases`

Fetch all available case studies

**Response:**

```json
{
  "cases": [
    {
      "id": "company-a",
      "name": "Company A: The Scaling Crisis",
      "description": "How rapid growth without infrastructure led to collapse",
      "dataPoints": 1247
    }
  ]
}
```

### `GET /api/cases/:caseId`

Fetch specific case details and context

---

## Real-World Example: Learning from Past Migrations

### Scenario: Data Migration Gone Right (and Wrong)

**Problem:** Your team needs to migrate 10M user records from MySQL to PostgreSQL. No one on the current team has done a migration of this scale, and the stakes are high—downtime means lost revenue.

**Query to DONNA:**

```
"Have we done large database migrations before? What challenges did we face?"
```

**DONNA's Response:**

> "Yes, two major migrations in company history:
>
> ### Migration 1: MongoDB → PostgreSQL (June 2022)
>
> **Scale:** 3M documents  
> **Duration:** 2 weeks  
> **Outcome:** Partially successful
>
> **What went wrong:**
>
> - Underestimated downtime: 6 hours instead of planned 2 hours
> - Didn't test rollback procedure - got stuck midway
> - Foreign key constraints caused unexpected failures
> - No read replica for fallback during migration
>
> **From postmortem:**
> 'We should have used a dual-write strategy with gradual cutover instead of big-bang migration.'
>
> ### Migration 2: PostgreSQL → PostgreSQL (New Schema) (March 2023)
>
> **Scale:** 5M rows  
> **Duration:** 4 days (zero downtime)  
> **Outcome:** Success
>
> **What worked:**
>
> 1. **Dual-write period:** Wrote to both old and new schema for 1 week
> 2. **Shadow traffic:** Replayed production queries against new DB
> 3. **Feature flags:** Gradual cutover by user percentage (10% → 50% → 100%)
> 4. **Automated rollback:** Script to switch back to old schema in < 5 minutes
> 5. **Data validation:** Checksums to verify data integrity
>
> **Key Tooling:**
>
> - Custom script: `scripts/db-migration/dual-writer.py`
> - Monitoring dashboard: Grafana alerts on query latency differences
>
> **From Tech Lead:**
> 'Never do big-bang migrations. Always have a rollback plan tested in staging. The dual-write strategy saved us.'
>
> **Recommended Reading:**
>
> - Internal wiki: 'Database Migration Playbook'
> - Slack conversation in #data-engineering (March 2023)
> - Contact @db-expert-mike for advice"

**Impact:**

- **Risk Mitigation:** Team uses proven dual-write strategy instead of dangerous big-bang approach
- **Zero Downtime:** Avoids 6-hour outage that would cost $50K+ in lost revenue
- **Confidence:** Team has a tested playbook and knows who to ask for help
- **Time Saved:** 3-7 days of research and planning

---

## Quantified Impact Across Use Cases

Organizations using DONNA report measurable ROI across common engineering scenarios:

| Scenario                   | Time Saved                    | Cost Avoided | Knowledge Preserved               |
| -------------------------- | ----------------------------- | ------------ | --------------------------------- |
| Avoiding Duplicate Work    | 2-3 weeks                     | $15K-30K     | ✅ Past implementations & lessons |
| Faster Incident Resolution | 2-4 hours                     | $5K-20K      | ✅ Root causes & fixes            |
| Architecture Decisions     | 1-2 months                    | $50K-100K    | ✅ Past attempts & trade-offs     |
| New Hire Onboarding        | 1-2 weeks                     | $8K-15K      | ✅ Context & reasoning            |
| Migration Planning         | 3-7 days                      | $10K-40K     | ✅ Proven playbooks               |
| Pre-Launch Risk Mitigation | 1 week + prevented incident   | $50K+        | ✅ Checklists & warnings          |
| Build vs Buy Decisions     | 3 months + ongoing            | $180K/year   | ✅ Cost analysis & outcomes       |
| Compliance Adherence       | 2 months + fines              | €15K+        | ✅ Processes & requirements       |
| Design System Consistency  | 1-3 days                      | $3K-8K       | ✅ Patterns & constraints         |
| Performance Optimization   | 1-3 weeks (redirected effort) | $10K-25K     | ✅ Prioritization framework       |
| Scalability Planning       | 2-4 months                    | $50K-100K    | ✅ Capacity & growth strategies   |

**Total Average Annual Savings Per Team:** $300K-500K  
**Intangible Benefits:** Faster onboarding, reduced risk, better decisions, preserved institutional culture

---

## Additional Example Queries

Each case workspace includes intelligent query suggestions:

**For a Failed Product Launch:**

- "What were the key warning signs before launch?"
- "How did internal communication break down?"
- "What technical debt contributed to the failure?"

**For a Scaling Crisis:**

- "When did the infrastructure problems start?"
- "What architectural decisions caused bottlenecks?"
- "How did the team respond to the outage?"

**For a Successful Pivot:**

- "What data informed the pivot decision?"
- "How did the team align on the new direction?"
- "What was the turning point in customer adoption?"

---

## Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

---

<div align="center">

**DONNA** - _Because your organization's past deserves a future_

⭐ Star this repo if you believe in preserving institutional knowledge!

</div>

