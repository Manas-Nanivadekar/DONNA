# DONNA

**Distributed Organizational Neural Network Assistant**

<div align="center">

*Transforming scattered organizational history into actionable institutional knowledge*

[![Next.js](https://img.shields.io/badge/Next.js-15-black?style=flat-square&logo=next.js)](https://nextjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat-square&logo=fastapi)](https://fastapi.tiangolo.com/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=flat-square&logo=typescript)](https://www.typescriptlang.org/)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat-square&logo=python)](https://www.python.org/)

</div>

---

## The Problem

Organizations suffer from **institutional amnesia**:

- **Repeated Mistakes**: Teams lack visibility into past failures and repeat them
- **Knowledge Loss**: When employees leave, critical context and reasoning disappears  
- **Slow Onboarding**: New hires can't understand why systems exist or how they evolved
- **Context Fragmentation**: Critical knowledge is scattered across Slack, GitHub, Jira, Confluence, and Docs

The question "Has anyone tried this before?" echoes through Slack channels, met with silence or vague memories.

---

## The Solution

**DONNA** is an AI-powered institutional memory assistant that captures, contextualizes, and surfaces historical organizational knowledge through conversational exploration of real case studies.

### How DONNA Helps

- **Learns from History**: Analyzes codebases, docs, tickets, and communication tools to understand what worked, what failed, and why
- **Understands Previous Work**: Builds a contextual map of past projects, showing how systems evolved and why certain choices were made
- **Warns Before Mistakes**: Proactively flags decisions similar to past failures with context, impact, and proven solutions
- **Preserves Organizational Wisdom**: Knowledge persists beyond employee tenure, new hires learn from years of experience from day one

---

## Features

### Case-Based Learning
Explore four interactive case workspaces, each representing a real company's historical data including Slack discussions, GitHub commits, Jira tickets, Confluence documentation, and Google Docs notes.

### Conversational AI
Ask DONNA anything about organizational history and receive contextual, AI-generated answers using the company's data as reference.

### Polished Interface
- Elegant black-and-white aesthetic
- Smooth animations via Framer Motion
- Mobile-responsive design
- Collapsible context panels
- Smart query suggestions

### Intelligent Context
- **Vector Search**: Semantic understanding via Qdrant vector database
- **Separate Chat Threads**: Each case maintains its own workspace memory
- **Streamed Responses**: Real-time AI generation for fluid interaction

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DONNA Frontend                      ‚îÇ
‚îÇ              Next.js 15 + TypeScript + Tailwind            ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   Landing   ‚îÇ  ‚îÇ     Case     ‚îÇ  ‚îÇ   Chat Thread   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    Page     ‚îÇ‚îÄ‚îÄ‚îÇ   Workspace  ‚îÇ‚îÄ‚îÄ‚îÇ   (per case)    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ HTTP/REST
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DONNA Backend                         ‚îÇ
‚îÇ                  FastAPI + Python 3.12                     ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   MongoDB    ‚îÇ  ‚îÇ    Qdrant    ‚îÇ  ‚îÇ    Ollama    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Metadata &  ‚îÇ  ‚îÇVector Search ‚îÇ  ‚îÇ  Embeddings  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  User Data   ‚îÇ  ‚îÇ (localhost)  ‚îÇ  ‚îÇ(nomic-embed) ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                    ‚îÇGoogle Gemini ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ  AI Engine   ‚îÇ                        ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tech Stack

**Frontend**
- Next.js 15 (App Router)
- React 18 + TypeScript
- Tailwind CSS
- Framer Motion
- AI SDK (Vercel)

**Backend**
- FastAPI
- Python 3.12
- MongoDB (company data, metadata)
- Qdrant (vector database)
- Ollama (nomic-embed-text embeddings)
- Google Gemini AI (chat generation)

---

## Getting Started

### Prerequisites

Ensure you have the following installed and running:
- Node.js 18+
- Python 3.12+
- MongoDB instance
- Qdrant vector database (localhost:6333)
- Ollama (with nomic-embed-text model)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Manas-Nanivadekar/DONNA.git
   cd DONNA
   git checkout dev
   ```

2. **Set up environment variables**

   Create a `.env.local` file in the root directory:
   ```env
   # Google Gemini AI
   GOOGLE_API_KEY=your_google_api_key_here
   
   # MongoDB
   MONGODB_URI=mongodb://localhost:27017
   MONGODB_DB_NAME=donna
   
   # Qdrant
   QDRANT_HOST=localhost
   QDRANT_PORT=6333
   
   # Ollama
   OLLAMA_BASE_URL=http://localhost:11434
   ```

3. **Install frontend dependencies**
   ```bash
   pnpm install
   # or
   npm install
   # or
   yarn install
   ```

4. **Install backend dependencies**
   ```bash
   # Create and activate virtual environment
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   
   # Install Python packages
   pip install -r requirements.txt
   ```

5. **Start required services**
   ```bash
   # Start MongoDB (if not already running)
   mongod
   
   # Start Qdrant
   docker run -p 6333:6333 qdrant/qdrant
   
   # Start Ollama
   ollama serve
   
   # Pull required embedding model
   ollama pull nomic-embed-text
   ```

6. **Run the application**
   ```bash
   pnpm dev
   ```

The application will be available at:
- Frontend: `http://localhost:3000`
- Backend API: `http://localhost:3000/api/*`

---

## üìÇ Project Structure

```
DONNA/
‚îú‚îÄ‚îÄ api/                          # FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ index.py                  # Main API endpoint
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # Qdrant vector operations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.py          # Ollama embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm.py                # Gemini AI integration
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ stream.py             # Response streaming
‚îÇ       ‚îú‚îÄ‚îÄ prompt.py             # Prompt engineering
‚îÇ       ‚îî‚îÄ‚îÄ attachment.py         # File handling
‚îÇ
‚îú‚îÄ‚îÄ app/                          # Next.js Frontend
‚îÇ   ‚îú‚îÄ‚îÄ (chat)/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx              # Main chat page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [caseId]/             # Dynamic case routes
‚îÇ   ‚îú‚îÄ‚îÄ cases/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx              # Case selection landing
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                # Root layout
‚îÇ   ‚îî‚îÄ‚îÄ api/                      # API route handlers
‚îÇ
‚îú‚îÄ‚îÄ components/                   # React Components
‚îÇ   ‚îú‚îÄ‚îÄ chat.tsx                  # Chat interface
‚îÇ   ‚îú‚îÄ‚îÄ case-card.tsx             # Case preview cards
‚îÇ   ‚îú‚îÄ‚îÄ context-panel.tsx         # Collapsible summary
‚îÇ   ‚îú‚îÄ‚îÄ suggestions-panel.tsx     # Query suggestions
‚îÇ   ‚îú‚îÄ‚îÄ message.tsx               # Chat message display
‚îÇ   ‚îú‚îÄ‚îÄ multimodal-input.tsx      # Chat input field
‚îÇ   ‚îî‚îÄ‚îÄ ui/                       # Reusable UI components
‚îÇ
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mongodb.ts            # MongoDB client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.ts             # Data models
‚îÇ   ‚îî‚îÄ‚îÄ utils.ts                  # Utility functions
‚îÇ
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îú‚îÄ‚îÄ use-chat-history.tsx      # Per-case chat state
‚îÇ   ‚îî‚îÄ‚îÄ use-scroll-to-bottom.tsx  # Auto-scroll
‚îÇ
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ cases/                    # Case metadata & assets
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ package.json                  # Node.js dependencies
‚îî‚îÄ‚îÄ vercel.json                   # Deployment config
```

---

## User Experience Flow

1. **Landing Page**: User sees four case study cards
2. **One-Time Registration**: User provides name and email (stored locally)
3. **Case Selection**: Click a case to enter its workspace
4. **Workspace Interface**:
   - **Left Panel**: Collapsible case summary with full historical context
   - **Center**: Chat interface for conversational exploration
   - **Right Panel**: Suggested queries that prefill the input
5. **Exploration**: Ask questions, receive AI-streamed answers based on company data
6. **Case Switching**: Each case maintains its own separate chat thread

---

## API Endpoints

### `POST /api/chat`
Main chat endpoint for conversational AI

**Request Body:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Why did Project Phoenix fail?"
    }
  ],
  "caseId": "company-a",
  "userId": "user-123"
}
```

**Response:**
Streamed text using Data Stream Protocol

### `GET /api/cases`
Fetch all available case studies

**Response:**
```json
{
  "cases": [
    {
      "id": "company-a",
      "name": "Company A: The Scaling Crisis",
      "description": "How rapid growth without infrastructure led to collapse",
      "dataPoints": 1247
    }
  ]
}
```

### `GET /api/cases/:caseId`
Fetch specific case details and context

---

## Real-World Example: Learning from Past Migrations

### Scenario: Data Migration Gone Right (and Wrong)

**Problem:** Your team needs to migrate 10M user records from MySQL to PostgreSQL. No one on the current team has done a migration of this scale, and the stakes are high‚Äîdowntime means lost revenue.

**Query to DONNA:**
```
"Have we done large database migrations before? What challenges did we face?"
```

**DONNA's Response:**

> "Yes, two major migrations in company history:
> 
> ### Migration 1: MongoDB ‚Üí PostgreSQL (June 2022)
> **Scale:** 3M documents  
> **Duration:** 2 weeks  
> **Outcome:** Partially successful
> 
> **What went wrong:**
> - Underestimated downtime: 6 hours instead of planned 2 hours
> - Didn't test rollback procedure - got stuck midway
> - Foreign key constraints caused unexpected failures
> - No read replica for fallback during migration
> 
> **From postmortem:**
> 'We should have used a dual-write strategy with gradual cutover instead of big-bang migration.'
> 
> ### Migration 2: PostgreSQL ‚Üí PostgreSQL (New Schema) (March 2023)
> **Scale:** 5M rows  
> **Duration:** 4 days (zero downtime)  
> **Outcome:** Success
> 
> **What worked:**
> 1. **Dual-write period:** Wrote to both old and new schema for 1 week
> 2. **Shadow traffic:** Replayed production queries against new DB
> 3. **Feature flags:** Gradual cutover by user percentage (10% ‚Üí 50% ‚Üí 100%)
> 4. **Automated rollback:** Script to switch back to old schema in < 5 minutes
> 5. **Data validation:** Checksums to verify data integrity
> 
> **Key Tooling:**
> - Custom script: `scripts/db-migration/dual-writer.py`
> - Monitoring dashboard: Grafana alerts on query latency differences
> 
> **From Tech Lead:**
> 'Never do big-bang migrations. Always have a rollback plan tested in staging. The dual-write strategy saved us.'
> 
> **Recommended Reading:**
> - Internal wiki: 'Database Migration Playbook'
> - Slack conversation in #data-engineering (March 2023)
> - Contact @db-expert-mike for advice"

**Impact:**
- ‚úÖ **Risk Mitigation:** Team uses proven dual-write strategy instead of dangerous big-bang approach
- ‚úÖ **Zero Downtime:** Avoids 6-hour outage that would cost $50K+ in lost revenue
- ‚úÖ **Confidence:** Team has a tested playbook and knows who to ask for help
- ‚úÖ **Time Saved:** 3-7 days of research and planning

---

## Quantified Impact Across Use Cases

Organizations using DONNA report measurable ROI across common engineering scenarios:

| Scenario | Time Saved | Cost Avoided | Knowledge Preserved |
|----------|------------|--------------|---------------------|
| Avoiding Duplicate Work | 2-3 weeks | $15K-30K | ‚úÖ Past implementations & lessons |
| Faster Incident Resolution | 2-4 hours | $5K-20K | ‚úÖ Root causes & fixes |
| Architecture Decisions | 1-2 months | $50K-100K | ‚úÖ Past attempts & trade-offs |
| New Hire Onboarding | 1-2 weeks | $8K-15K | ‚úÖ Context & reasoning |
| Migration Planning | 3-7 days | $10K-40K | ‚úÖ Proven playbooks |
| Pre-Launch Risk Mitigation | 1 week + prevented incident | $50K+ | ‚úÖ Checklists & warnings |
| Build vs Buy Decisions | 3 months + ongoing | $180K/year | ‚úÖ Cost analysis & outcomes |
| Compliance Adherence | 2 months + fines | ‚Ç¨15K+ | ‚úÖ Processes & requirements |
| Design System Consistency | 1-3 days | $3K-8K | ‚úÖ Patterns & constraints |
| Performance Optimization | 1-3 weeks (redirected effort) | $10K-25K | ‚úÖ Prioritization framework |
| Scalability Planning | 2-4 months | $50K-100K | ‚úÖ Capacity & growth strategies |

**Total Average Annual Savings Per Team:** $300K-500K  
**Intangible Benefits:** Faster onboarding, reduced risk, better decisions, preserved institutional culture

---

## üß™ Additional Example Queries

Each case workspace includes intelligent query suggestions:

**For a Failed Product Launch:**
- "What were the key warning signs before launch?"
- "How did internal communication break down?"
- "What technical debt contributed to the failure?"

**For a Scaling Crisis:**
- "When did the infrastructure problems start?"
- "What architectural decisions caused bottlenecks?"
- "How did the team respond to the outage?"

**For a Successful Pivot:**
- "What data informed the pivot decision?"
- "How did the team align on the new direction?"
- "What was the turning point in customer adoption?"

---

## Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

---

<div align="center">

**DONNA** - *Because your organization's past deserves a future*

‚≠ê Star this repo if you believe in preserving institutional knowledge!

</div>
